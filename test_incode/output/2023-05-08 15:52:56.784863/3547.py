#!/usr/bin/python3
url = "http://snirps.ddns.net"
if __name__ == '__main__':
    #A endless loop that iterates over a http request.
    while True:
        req = requests.get(url)
        soup = BeautifulSoup(req.text, 'html.parser')
        
        #Print the title of the page.
        print(soup.title.string)
        
        #Print the links of the page.
        for link in soup.find_all('a'):
            print(link.get('href'))
        
        #Wait for another page to load.
        req = requests.get(url)
        soup = BeautifulSoup(req.text, 'html.parser')
        
        #Print the title of the page.
        print(soup.title.string)
        
        #Print the links of the page.
        for link in soup.find_all('a'):
            print(link.get('href'))
        
        #Wait for another page to load.
        req = requests.get(url)
        soup = BeautifulSoup(req.text, 'html.parser')
        
        #Print the title of the page.
        print(soup.title.string)
        
        #Print the links of the page.
        for link in soup.find_all('a'):
            print(link.get('href'))
        
        #Wait for another page to load.
        req = requests.get(url)
        soup = BeautifulSoup(req.text, 'html.parser')
        
        #Print the title of the page.
        print(soup.title.string)
        
        #Print the links of the page.
        for link in soup.find_all('a'):
            print(link.get('href'))
        
        #Wait for another page to load.
        req = requests.get(url)
        soup = BeautifulSoup(req.text, 'html.parser')
        
        #Print the title of the page.
        print(soup.title.string)
        
        #Print the links of the page.
        for link in soup.find_all('a'):
            print(link.get('href'))
        
        #Wait for another page to load.
        req = requests.get(url)
        soup = BeautifulSoup(req.text, 'html.parser')
        
        #Print the title of the page.
        print(soup.title.string)
        
        #Print the links of the page.
        for link in soup.find_all('a'):
            print(link.get('href'))
        
        #Wait for another page to load.
        req = requests.get(url)
        soup = BeautifulSoup(req.text, 'html.parser')
        
        #Print the title of the page.
        print(soup.title.string)
        
        #Print the links of the page.
        for link in soup.find_all('a'):
            print(link.get('href'))
        
        #Wait for another page to load.
        req = requests.get(url)
        soup = BeautifulSoup(req.text, 'html.parser')
        
        #Print the title of the page.
        print(soup.title.string)
        
        #Print the links of the page.
        for link in soup.find_all('a'):
            print(link.get('href'))
        
        #Wait for another page to load.
        req = requests.get(url)
        soup = BeautifulSoup(req.text, 'html.parser')
        
        #Print the title of the page.
        print(soup.title.string)
        
        #Print the links of the page.
        for link in soup.find_all('a'):
            print(link.get('href'))
        
        #Wait for another page to load.
        req = requests.get(url)
        soup = BeautifulSoup(req.text, 'html.parser')
        
        #Print the title of the page.
        print(soup.title.string)
        
        #Print the links of the page.
        for link in soup.find_all('a'):
            print(link.get('href'))
        
        #Wait for another page to load.
        req = requests.get(url)
        soup = BeautifulSoup(req.text, 'html.parser')
        
        #Print the title of the page.
        print(soup.title.string)
        
        #Print the links of the page.
        for link in soup.find_all('a'):
            print(link.get('href'))
        
        #Wait for another page to load.
        req = requests.get(url)
        soup = BeautifulSoup(req.text, 'html.parser')
        
        #Print the title of the page.
        print(soup.title.string)
        
        #Print the links of the page.
        for link in soup.find_all('a'):
            print(link.get('href'))
        
        #Wait for another page to load.
        req = requests.get(url)
        soup = BeautifulSoup(req.text, 'html.parser')
        
        #Print the title of the page.
        print(soup.title.string)
        
        #Print the links of the page.
        for link in soup.find_all('a'):
            print(link.get('href'))
        
        #Wait for another page to load.
        req = requests.get(url)
        soup = BeautifulSoup(req.text, 'html.parser')
        
        #Print the title of the page.
        print(soup.title.string)
        
        #Print the links of the page.
        for link in soup.find_all('a'):
            print(link.get('href'))
        
        #Wait for another page to load.
        req = requests.get(url)
        soup = BeautifulSoup(req.text, 'html.parser')
        
        #Print the title of the page.
        print(soup.title.string)
        
        #Print the links of the page.
        for link in soup.find_all('a'):
            print(link.get('href'))
        
        #Wait for another page to load.
        req = requests.get(url)
        soup = BeautifulSoup(req.text, 'html.parser')
        
        #Print the title of the page.
        print(soup.title.string)
        
        #Print the links of the page.
        for link in soup.find_all('a'):
            print(link.get('href'))
        
        #Wait for another page to load.
        req = requests.get(url)
        soup = BeautifulSoup(req.text, 'html.parser')
        
        #Print the title of the page.
        print(soup.title.string)
        
        #Print the links of the page.
        for link in soup.find_all('a'):
            print(link.get('href'))
        
        #Wait for another page to load.
        

